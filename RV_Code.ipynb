{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from scipy.io import loadmat\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'F:\\\\RV System\\\\RV_Systems_ML_Training_Sets\\\\S1.dat.mat'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\scipy\\io\\matlab\\_mio.py:39\u001b[0m, in \u001b[0;36m_open_file\u001b[1;34m(file_like, appendmat, mode)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 39\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mopen\u001b[39;49m(file_like, mode), \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     41\u001b[0m     \u001b[39m# Probably \"not found\"\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'F:\\\\RV System\\\\RV_Systems_ML_Training_Sets\\\\S1.dat'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, \u001b[39m13\u001b[39m):  \u001b[39m# Assuming there are 12 files (S1 to S12)\u001b[39;00m\n\u001b[0;32m     10\u001b[0m     file_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(dataset_dir, \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mS\u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m}\u001b[39;00m\u001b[39m.dat\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m---> 11\u001b[0m     data \u001b[39m=\u001b[39m loadmat(file_path)\n\u001b[0;32m     13\u001b[0m     \u001b[39m# Extract and store vibration data and corresponding labels\u001b[39;00m\n\u001b[0;32m     14\u001b[0m     \u001b[39mif\u001b[39;00m i \u001b[39min\u001b[39;00m [\u001b[39m1\u001b[39m, \u001b[39m8\u001b[39m, \u001b[39m12\u001b[39m]:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\scipy\\io\\matlab\\_mio.py:225\u001b[0m, in \u001b[0;36mloadmat\u001b[1;34m(file_name, mdict, appendmat, **kwargs)\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     89\u001b[0m \u001b[39mLoad MATLAB file.\u001b[39;00m\n\u001b[0;32m     90\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    222\u001b[0m \u001b[39m    3.14159265+3.14159265j])\u001b[39;00m\n\u001b[0;32m    223\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    224\u001b[0m variable_names \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m'\u001b[39m\u001b[39mvariable_names\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m--> 225\u001b[0m \u001b[39mwith\u001b[39;00m _open_file_context(file_name, appendmat) \u001b[39mas\u001b[39;00m f:\n\u001b[0;32m    226\u001b[0m     MR, _ \u001b[39m=\u001b[39m mat_reader_factory(f, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    227\u001b[0m     matfile_dict \u001b[39m=\u001b[39m MR\u001b[39m.\u001b[39mget_variables(variable_names)\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_3.9.3568.0_x64__qbz5n2kfra8p0\\lib\\contextlib.py:119\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__enter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[39mdel\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkwds, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunc\n\u001b[0;32m    118\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 119\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mnext\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgen)\n\u001b[0;32m    120\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n\u001b[0;32m    121\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mgenerator didn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt yield\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\scipy\\io\\matlab\\_mio.py:17\u001b[0m, in \u001b[0;36m_open_file_context\u001b[1;34m(file_like, appendmat, mode)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[39m@contextmanager\u001b[39m\n\u001b[0;32m     16\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_open_file_context\u001b[39m(file_like, appendmat, mode\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m---> 17\u001b[0m     f, opened \u001b[39m=\u001b[39m _open_file(file_like, appendmat, mode)\n\u001b[0;32m     18\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     19\u001b[0m         \u001b[39myield\u001b[39;00m f\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\scipy\\io\\matlab\\_mio.py:45\u001b[0m, in \u001b[0;36m_open_file\u001b[1;34m(file_like, appendmat, mode)\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[39mif\u001b[39;00m appendmat \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m file_like\u001b[39m.\u001b[39mendswith(\u001b[39m'\u001b[39m\u001b[39m.mat\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m     44\u001b[0m         file_like \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m.mat\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m---> 45\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mopen\u001b[39;49m(file_like, mode), \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m     46\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     47\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mOSError\u001b[39;00m(\n\u001b[0;32m     48\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mReader needs file name or open file-like object\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m     49\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'F:\\\\RV System\\\\RV_Systems_ML_Training_Sets\\\\S1.dat.mat'"
     ]
    }
   ],
   "source": [
    "# Define the directory containing your dataset\n",
    "dataset_dir = r'F:\\RV System\\RV_Systems_ML_Training_Sets'\n",
    "\n",
    "# Initialize lists to store data and labels\n",
    "data_list = []\n",
    "labels_list = []\n",
    "\n",
    "# Load data from .dat files\n",
    "for i in range(1, 13):  # Assuming there are 12 files (S1 to S12)\n",
    "    file_path = os.path.join(dataset_dir, f'S{i}.dat')\n",
    "    data = loadmat(file_path)\n",
    "    \n",
    "    # Extract and store vibration data and corresponding labels\n",
    "    if i in [1, 8, 12]:\n",
    "        vibration_data = np.array(data['data'], dtype=np.complex128)\n",
    "    else:\n",
    "        vibration_data = np.array(data['data'], dtype=np.float64)\n",
    "    \n",
    "    data_list.append(vibration_data)\n",
    "    labels_list.append(np.full(vibration_data.shape[0], i))  # Label each dataset as S1, S2, ...\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "X = np.vstack(data_list)\n",
    "y = np.concatenate(labels_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing values (if any)\n",
    "X = np.nan_to_num(X)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Feature extraction: Statistical moments\n",
    "def extract_features(data):\n",
    "    features = []\n",
    "    for i in range(data.shape[0]):\n",
    "        row = data[i]\n",
    "        mean = np.mean(row)\n",
    "        var = np.var(row)\n",
    "        skew = pd.Series(row).skew()\n",
    "        kurt = pd.Series(row).kurtosis()\n",
    "        features.append([mean, var, skew, kurt])\n",
    "    return np.array(features)\n",
    "\n",
    "X_features = extract_features(X_scaled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA for dimensionality reduction\n",
    "pca = PCA(n_components=10)  # Adjust number of components as needed\n",
    "X_pca = pca.fit_transform(X_features)\n",
    "\n",
    "# Visualize explained variance\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Explained Variance')\n",
    "plt.title('PCA - Explained Variance')\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# SVM Model\n",
    "svm = SVC(kernel='rbf', C=1.0, gamma='auto')\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_svm = svm.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "print(\"SVM Accuracy:\", accuracy_score(y_test, y_pred_svm))\n",
    "print(confusion_matrix(y_test, y_pred_svm))\n",
    "print(classification_report(y_test, y_pred_svm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-NN Model\n",
    "knn = KNeighborsClassifier(n_neighbors=5, metric='euclidean')\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_knn = knn.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "print(\"k-NN Accuracy:\", accuracy_score(y_test, y_pred_knn))\n",
    "print(confusion_matrix(y_test, y_pred_knn))\n",
    "print(classification_report(y_test, y_pred_knn))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree Model\n",
    "tree = DecisionTreeClassifier(max_depth=10, min_samples_split=5)\n",
    "tree.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_tree = tree.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "print(\"Decision Tree Accuracy:\", accuracy_score(y_test, y_pred_tree))\n",
    "print(confusion_matrix(y_test, y_pred_tree))\n",
    "print(classification_report(y_test, y_pred_tree))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare models using cross-validation\n",
    "models = {\n",
    "    'SVM': svm,\n",
    "    'k-NN': knn,\n",
    "    'Decision Tree': tree\n",
    "}\n",
    "\n",
    "cv_scores = {}\n",
    "for name, model in models.items():\n",
    "    scores = cross_val_score(model, X_pca, y, cv=5)\n",
    "    cv_scores[name] = np.mean(scores)\n",
    "    print(f\"{name} CV Score: {np.mean(scores)}\")\n",
    "\n",
    "# Visualize cross-validation scores\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.barplot(x=list(cv_scores.keys()), y=list(cv_scores.values()))\n",
    "plt.title('Cross-Validation Scores')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Model')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid Search for SVM\n",
    "param_grid = {'C': [0.1, 1, 10, 100], 'gamma': ['scale', 'auto'], 'kernel': ['rbf', 'poly']}\n",
    "grid = GridSearchCV(SVC(), param_grid, refit=True, verbose=2, cv=5)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters and model evaluation\n",
    "print(\"Best Parameters for SVM:\", grid.best_params_)\n",
    "best_svm = grid.best_estimator_\n",
    "\n",
    "# Predictions and Evaluation\n",
    "y_pred_best_svm = best_svm.predict(X_test)\n",
    "print(\"Tuned SVM Accuracy:\", accuracy_score(y_test, y_pred_best_svm))\n",
    "print(confusion_matrix(y_test, y_pred_best_svm))\n",
    "print(classification_report(y_test, y_pred_best_svm))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the best model based on accuracy\n",
    "best_model = max(cv_scores, key=cv_scores.get)\n",
    "print(f\"The best model is: {best_model} with a cross-validation score of {cv_scores[best_model]}\")\n",
    "\n",
    "# Further reporting steps can be documented and visualized as needed.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
